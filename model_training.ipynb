{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Web Classification\n",
    "During my internship at Jewel Paymentech, I undertook an independent data science project to build a machine learning model that performs multi-class classfication of websites based on its text data.\n",
    "\n",
    "## Project Objective\n",
    "The objective of this project is to assist our clients in identifying various categories of potentially illegal or fraudulent website content such as pornography, gambling and multi-level marketing (based on the website's text content).\n",
    "\n",
    "## Machine Learning Pipeline\n",
    "This is an overview of a typical machine learning pipeline. To prepare my data for model training, data cleaning and text preprocessing will be performed to ensure the data is of 'good quality'.\n",
    "\n",
    "<img src=\"machine_learning_pipeline.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from collections import Counter\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sklearn Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Imbalanced-Learn Libraries\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Text Data from File Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PORN = 'Porn'\n",
    "PROSTITUTION = 'Prostitution'\n",
    "WEAPON = 'Weapon'\n",
    "TMS = 'Tms'\n",
    "TRIAL = 'Trial'\n",
    "PENNY = 'Penny'\n",
    "COUNTERFEIT = 'Counterfeit'\n",
    "CYBERLOCKER = 'Cyberlocker'\n",
    "MLM = 'Mlm'\n",
    "PHARMA = 'Pharma'\n",
    "SMOKE = 'Smoke'\n",
    "SPAM = 'Spam'\n",
    "CASINO = 'Casino'\n",
    "ALCOHOL = 'Alcohol'\n",
    "NORMAL = 'Normal'\n",
    "\n",
    "path = '../' + dataset_filepath\n",
    "\n",
    "SOURCES = [\n",
    "    (path + '/PORN',         PORN),\n",
    "    (path + '/PROSTITUTION', PROSTITUTION),\n",
    "    (path + '/WEAPON',       WEAPON),\n",
    "    (path + '/TMS',          TMS),\n",
    "    (path + '/TRIAL',        TRIAL),\n",
    "    (path + '/PENNY',        PENNY),\n",
    "    (path + '/COUNTERFEIT',  COUNTERFEIT),\n",
    "    (path + '/CYBERLOCKER',  CYBERLOCKER),\n",
    "    (path + '/MLM',          MLM),\n",
    "    (path + '/PHARMA',       PHARMA),\n",
    "    (path + '/SMOKE',        SMOKE),\n",
    "    (path + '/SPAM',         SPAM),\n",
    "    (path + '/CASINO',       CASINO),\n",
    "    (path + '/ALCOHOL',      ALCOHOL),\n",
    "    (path + '/NORMAL',       NORMAL)\n",
    "]\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):                \n",
    "                    lines = []\n",
    "                    # Use io.open instead for Python 2.0, open()takes no encoding argument. The third argument is the buffering option instead.\n",
    "                    f = io.open(file_path, encoding = 'latin-1')\n",
    "                    for line in f:\n",
    "                        lines.append(line)\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name[38:])\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "d = ({'text':[], 'class':[]})\n",
    "\n",
    "df = DataFrame(data = d)\n",
    "\n",
    "for path, classification in SOURCES:\n",
    "    df = df.append(build_data_frame(path, classification))\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7181 entries, dataset/balance_hotel_ovr_560sites/WEAPON/www.edbrown.com copy 2.txt to dataset/balance_hotel_ovr_560sites/WEAPON/www.aledge.com copy 3.txt\n",
      "Data columns (total 2 columns):\n",
      "class    7181 non-null object\n",
      "text     7181 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 168.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2366"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2366 entries, dataset/balance_hotel_ovr_560sites/WEAPON/www.edbrown.com copy 2.txt to dataset/balance_hotel_ovr_560sites/WEAPON/www.aledge.com copy 3.txt\n",
      "Data columns (total 2 columns):\n",
      "class    2366 non-null object\n",
      "text     2366 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 55.5+ KB\n"
     ]
    }
   ],
   "source": [
    "new_df = df.drop_duplicates(subset='text')\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class      15\n",
      "text     2366\n",
      "dtype: int64\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(new_df.nunique())\n",
    "print()\n",
    "print(new_df.duplicated().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal          570\n",
       "Porn            188\n",
       "Pharma          183\n",
       "Casino          180\n",
       "Alcohol         172\n",
       "Smoke           169\n",
       "Mlm             155\n",
       "Weapon          138\n",
       "Penny           136\n",
       "Spam            121\n",
       "Counterfeit     100\n",
       "Cyberlocker      85\n",
       "Prostitution     73\n",
       "Tms              63\n",
       "Trial            33\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAF3CAYAAACR0degAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmcbGddJ/7PN7lssiUkF4QsXCRRNjFgZNgcEFABxbCqmSABwQz+kAwyOIOOS2TUgZHNJA6ILEmQRSQEwg4GArKFJGRlkwiBxIQsEjYJSJLn98d5mlu36e5b996urj633+/Xq199zqlTp75PnaqnzqfOUtVaCwAAAIzVHvMuAAAAAHaFYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqG2adwG7Yt99921btmyZdxkAAADMwNlnn311a23z9uYbdbDdsmVLzjrrrHmXAQAAwAxU1Zenmc+hyAAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIzapnkXAAAAwK654tjT513CLrnd0Q/epfvbYwsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqM002FbVxVV1QVWdW1Vn9Wm3qar3V9UX+v+9+/SqqmOr6qKqOr+q7j3L2gAAANg9rMUe259rrR3SWju0jz83yWmttYOTnNbHk+QRSQ7uf0cledka1AYAAMDIzeNQ5MOSnNiHT0zy6InpJ7XBJ5LsVVW3n0N9AAAAjMisg21L8r6qOruqjurTbtdauzxJ+v/b9un7Jblk4r6X9mkAAACwrE0zXv4DWmuXVdVtk7y/qj63wry1xLT2QzMNAfmoJDnwwANXp0oAAABGa6Z7bFtrl/X/VyY5Jcl9klyxcIhx/39ln/3SJAdM3H3/JJctscxXtNYOba0dunnz5lmWDwAAwAjMLNhW1c2r6pYLw0l+IcmFSU5NcmSf7cgkb+vDpyZ5Ur868n2TfGPhkGUAAABYziwPRb5dklOqauFxXt9ae09VnZnkTVX11CRfSfKEPv+7kjwyyUVJvpPkKTOsDQAAgN3EzIJta+2LSX5qien/luShS0xvSZ4xq3oAAADYPc3j534AAABg1Qi2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIzazINtVe1ZVedU1Tv6+J2q6oyq+kJV/X1V3bhPv0kfv6jfvmXWtQEAADB+a7HH9r8l+ezE+AuSvKS1dnCSa5I8tU9/apJrWmsHJXlJnw8AAABWNNNgW1X7J/mlJK/s45XkIUne3Gc5Mcmj+/BhfTz99of2+QEAAGBZs95j+9Ik/yPJDX18nyRfb61d18cvTbJfH94vySVJ0m//Rp8fAAAAljWzYFtVv5zkytba2ZOTl5i1TXHb5HKPqqqzquqsq666ahUqBQAAYMxmucf2AUl+paouTvLGDIcgvzTJXlW1qc+zf5LL+vClSQ5Ikn77rZN8bfFCW2uvaK0d2lo7dPPmzTMsHwAAgDGYWbBtrf1+a23/1tqWJL+e5AOttSOSfDDJ4/tsRyZ5Wx8+tY+n3/6B1toP7bEFAACASfP4Hdv/meTZVXVRhnNoX9WnvyrJPn36s5M8dw61AQAAMDKbtj/LrmutnZ7k9D78xST3WWKe7yZ5wlrUAwAAwO5jHntsAQAAYNUItgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIzaVMG2qk6bZhoAAACstU0r3VhVN03yI0n2raq9k1S/6VZJ7jDj2gAAAGC7Vgy2Sf5rkmdlCLFnZ2uw/WaSv55hXQAAADCVFYNta+2vkvxVVT2ztXbcGtUEAAAAU9veHtskSWvtuKq6f5Itk/dprZ00o7oAAABgKlMF26p6bZI7Jzk3yfV9cksi2AIAADBXUwXbJIcmuVtrrc2yGAAAANhR0/6O7YVJfnRHFlxVN62qT1bVeVX16ar60z79TlV1RlV9oar+vqpu3KffpI9f1G/fsiOPBwAAwMY0bbDdN8lnquq9VXXqwt927vO9JA9prf1UkkOSPLyq7pvkBUle0lo7OMk1SZ7a539qkmtaawcleUmfDwAAAFY07aHIx+zogvthy9/uozfqfy3JQ5L8lz79xL7slyU5bOJx3pzk+Koqhz8DAACwkmmvivyhnVl4Ve2Z4fdvD8rwu7f/kuTrrbXr+iyXJtmvD++X5JL+eNdV1TeS7JPk6p15bAAAADaGqQ5FrqpvVdU3+993q+r6qvrm9u7XWru+tXZIkv2T3CfJXZeabeFhVrhtspajquqsqjrrqquumqZ8AAAAdmNTBdvW2i1ba7fqfzdN8rgkx0/7IK21ryc5Pcl9k+xVVQt7ivdPclkfvjTJAUnSb791kq8tsaxXtNYOba0dunnz5mlLAAAAYDc17cWjttFae2uGc2WXVVWbq2qvPnyzJA9L8tkkH0zy+D7bkUne1odP7ePpt3/A+bUAAABsz1Tn2FbVYydG98jwu7bbC523T3JiP892jyRvaq29o6o+k+SNVfVnSc5J8qo+/6uSvLaqLsqwp/bXp28GAAAAG9W0V0V+1MTwdUkuznAV42W11s5Pcq8lpn8xw/m2i6d/N8kTpqwHAAAAkkx/VeSnzLoQAAAA2BnTXhV5/6o6paqurKorqurkqtp/1sUBAADA9kx78ajXZLi40x0y/N7s2/s0AAAAmKtpg+3m1tprWmvX9b8TkvitHQAAAOZu2mB7dVU9sar27H9PTPJvsywMAAAApjFtsP3NJL+a5KtJLs/wO7MuKAUAAMDcTftzP/87yZGttWuSpKpuk+SFGQIvAAAAzM20e2zvuRBqk6S19rUs8Ru1AAAAsNamDbZ7VNXeCyN9j+20e3sBAABgZqYNpy9K8rGqenOSluF82z+fWVUAAAAwpamCbWvtpKo6K8lDklSSx7bWPjPTygAAAGAKUx9O3IOsMAsAAMC6Mu05tgAAALAuCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjNrMgm1VHVBVH6yqz1bVp6vqv/Xpt6mq91fVF/r/vfv0qqpjq+qiqjq/qu49q9oAAADYfcxyj+11Sf57a+2uSe6b5BlVdbckz01yWmvt4CSn9fEkeUSSg/vfUUleNsPaAAAA2E3MLNi21i5vrX2qD38ryWeT7JfksCQn9tlOTPLoPnxYkpPa4BNJ9qqq28+qPgAAAHYPa3KObVVtSXKvJGckuV1r7fJkCL9Jbttn2y/JJRN3u7RPAwAAgGXNPNhW1S2SnJzkWa21b6406xLT2hLLO6qqzqqqs6666qrVKhMAAICRmmmwraobZQi1r2utvaVPvmLhEOP+/8o+/dIkB0zcff8kly1eZmvtFa21Q1trh27evHl2xQMAADAKs7wqciV5VZLPttZePHHTqUmO7MNHJnnbxPQn9asj3zfJNxYOWQYAAIDlbJrhsh+Q5DeSXFBV5/Zpf5Dk+UneVFVPTfKVJE/ot70rySOTXJTkO0meMsPaAAAA2E3MLNi21j6Spc+bTZKHLjF/S/KMWdUDAADA7mlNrooMAAAAszLLQ5Hn5qqX/d28S9glm3/7ifMuAQAAYDTssQUAAGDUBFsAAABGbbc8FBkAYLU95uSPzLuEnXbK4x447xIAZsoeWwAAAEbNHlsAANggznnllfMuYZfc62m3nXcJrFP22AIAADBqgi0AAACjJtgCAAAwas6xBQAAdktffeFF8y5hp/3ocw6adwmjYo8tAAAAo2aPLQAA2zj6lEvmXcIuOfYxB8y7BGCN2WMLAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmp/7AQB2yqPe/JZ5l7BL3v74x867BABWiT22AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIzapnkXADvi46/45XmXsEvud9Q75l0CAADsdgRbgDl4yikPn3cJO+01j3nPvEsAANiGQ5EBAAAYNcEWAACAURNsAQAAGDXBFgAAgFETbAEAABg1wRYAAIBRE2wBAAAYNb9jCwDAhvaWN1897xJ22mMfv++8S4B1wR5bAAAARk2wBQAAYNQEWwAAAEZNsAUAAGDUBFsAAABGTbAFAABg1GYWbKvq1VV1ZVVdODHtNlX1/qr6Qv+/d59eVXVsVV1UVedX1b1nVRcAAAC7l1nusT0hycMXTXtuktNaawcnOa2PJ8kjkhzc/45K8rIZ1gUAAMBuZGbBtrX24SRfWzT5sCQn9uETkzx6YvpJbfCJJHtV1e1nVRsAAAC7j7U+x/Z2rbXLk6T/v22fvl+SSybmu7RPAwAAgBVtmncBXS0xrS05Y9VRGQ5XzoEHHjjLmgBYBY885c/mXcIueddj/nDeJQAA27HWe2yvWDjEuP+/sk+/NMkBE/Ptn+SypRbQWntFa+3Q1tqhmzdvnmmxAAAArH9rHWxPTXJkHz4yydsmpj+pXx35vkm+sXDIMgAAAKxkZociV9Ubkjw4yb5VdWmSP0ny/CRvqqqnJvlKkif02d+V5JFJLkrynSRPmVVdAAAA7F5mFmxba4cvc9NDl5i3JXnGrGoBAABg97XWhyIDAADAqhJsAQAAGDXBFgAAgFFbL79jCwC7hV86+ZXzLmGnvfNxT5t3CQCwU+yxBQAAYNQEWwAAAEZNsAUAAGDUBFsAAABGTbAFAABg1ARbAAAARs3P/ewGLvvrZ8+7hJ12h2e8eN4lAAAAI2ePLQAAAKMm2AIAADBqgi0AAACjJtgCAAAwai4eBevYm1/z8HmXsNMe/5T3zLsEAAA2CHtsAQAAGDXBFgAAgFETbAEAABg1wRYAAIBRE2wBAAAYNcEWAACAURNsAQAAGDXBFgAAgFETbAEAABg1wRYAAIBRE2wBAAAYNcEWAACAURNsAQAAGDXBFgAAgFETbAEAABg1wRYAAIBR2zTvAgCS5NjX/eK8S9glRx/x3nmXAACwYdljCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqAm2AAAAjJpgCwAAwKitq2BbVQ+vqs9X1UVV9dx51wMAAMD6t26CbVXtmeSvkzwiyd2SHF5Vd5tvVQAAAKx36ybYJrlPkotaa19srf1HkjcmOWzONQEAALDOradgu1+SSybGL+3TAAAAYFnVWpt3DUmSqnpCkl9srT2tj/9Gkvu01p65aL6jkhzVR38iyefXtNDBvkmunsPjzsNGamuysdq7kdqabKz2bqS2JhurvRuprYn27s42UluTjdXejdTWZGO1d15tvWNrbfP2Ztq0FpVM6dIkB0yM75/kssUztdZekeQVa1XUUqrqrNbaofOsYa1spLYmG6u9G6mtycZq70Zqa7Kx2ruR2ppo7+5sI7U12Vjt3UhtTTZWe9d7W9fTochnJjm4qu5UVTdO8utJTp1zTQAAAKxz62aPbWvtuqr6nSTvTbJnkle31j4957IAAABY59ZNsE2S1tq7krxr3nVMYa6HQq+xjdTWZGO1dyO1NdlY7d1IbU02Vns3UlsT7d2dbaS2JhurvRuprcnGau+6buu6uXgUAAAA7Iz1dI4tAAAA7DDBdkJVXV9V51bVhVX1D1X1I/OuaSmzqLOqnlxVd5gYf2VV3a0P/8GuLmNWquolVfWsifH3VtUrJ8ZfVFXPnmUN601V7dNfH+dW1Ver6l8nxm887/pW0460tb82brmd5V1aVXvNturpjaVPWkpV/WhVvbGq/qWqPlNV76qqH1/F5T+4qu6/k/d9Q1WdX1W/u8I8T6+qJ/Xhbfq2RfNN3c6+nON3puaJZZxeVTt8RcqqOqaqnrMrj72aqqpV1WsnxjdV1VVV9Y4+vsvP1VpZ6n1aVVuq6sJ51zZrVfW/qurT/f10blX9p11c3oMXXgPr2Wq3ez1ajT58LbYDd1VVPab3R3fp4zv93q2qi6tq3x2Yf037ud7OF02MP6eqjlmrx++PeUJVPX5Wyxdst3Vta+2Q1to9kvxHkqdPc6carOVzuWKdO1nPk5P8YMOttfa01tpn+uhUwXY7y5iVjyW5f5L0Nu+b5O4Tt98/yUdnXMO60lr7t/76OCTJy5O8ZGG8tfYf865vNU3T1oX3Q2vtF1tr35pvxTtsp/qkeauqSnJKktNba3durd0tQz9yu1V8mAenv/d3oK5NVfWjSe7fWrtna+0ly83bWnt5a+2kPvrkTPRtE8tbi3ZOPt6es1juFI87i+tx/HuSe1TVzfr4zyf51xk8zlpY9ffpjJ7zVVVV90vyy0nu3Vq7Z5KHJblkvlXN3kZo92r1bWu0HbirDk/ykQy/xrK7+16Sx+5I+J40hn5JsF3ePyU5KEmq6tn9m9gLq+8d7N/ofLaq/l+STyU5oKq+XVV/XlXnVdUnqmomGzdL1blMPYdX1QW97hf0uvfs35Zc2G/73f7NyaFJXte/ebzZwl6Bqnp+kpv16a9b/E3Wwrc9Ky2jz/dDtfTpu/qcfTRbN27vnuTCJN+qqr2r6iZJ7prknKr6vao6s3+7+qcTj//Wqjq7f/N61KK6XlRVn6qq06pqc59+SK/z/Ko6par27tNPr6oXVNUnq+qfq+pnd7AdM9fX3ef6N6gX9vX5sKr6aFV9oaru0+d7UG3d63lObWcv53pUVQf1Nr48w/vh9jWxN7aq3j6x3p8232qnNtknPbG/1s6tqr9ZCDzLvZ/6e/7YqvpYVX2xv19TVa+tqsMWHqC/Jn5lFWr9uSTfb629fGFCa+3cJB+pqr+c6H9+rT/uNntpqur4qnpyH764qv60vxcvqKq7VNWWDOHhd/tz8LNVtbmqTu7v8zOr6gH9/sdU1Suq6n1JTkryviS3nbjfnavqPf318E+19Vv7Y3r/9kN92xTtPGqF5/WA/nifr6o/mZhnpXX6vKo6I8n9Jp/kFfrVh/fn67yqOm3xyqmq36qqd9fQTy/X/hOq6sVV9cEkL1i8jFXy7iS/1IcPT/KGpWbqtbysqj7YX78PqqpX1/CZd8KMattZP3ifJtmzqv629zPvW3jt9Of/zL5+Tq5+JMbi57y/Bk/s9724qh5bVf+3r/P3VNWN+v3+uC/vwv5arzVq6+2TXN1a+16StNaubq1d1mv9i6r6eFWdVVX3ruGImX+pqqf3mmupvmBSVf1MDZ9BP1ZVN+/r/Mw+7bDF86+hldq9sB3wyapa6K8fVVVn9Lr/sbb2y1Ot3zlZrm87p4ZtooX++LAk6evnnf01fWFt7dsntwOX+3y6Y1/m+f3/gWvVyKq6RZIHJHlqlgi2NWwzv7C39fyqemaf/tC+Pi/or8ubTNztmRPPz0J/epsatjfP722/51q0bwnXZbj40w8drbTcehhdv9Ra89f/kny7/9+U5G1JfjvJTye5IMnNk9wiyaeT3CvJliQ3JLnvxP1bkkf14f+b5A/XsM5t6smwd+ErSTb3+T6Q5NG9Pe+fWNZe/f/pSQ6dmP6D8YXH68Nbklw4Mf6cJMestIzlalmt5yzJxUkOTPJfM2zs/u8kj8zQWX04yS9keCNXhi9z3pHkP/f73qb/v1mGULzPRF1H9OE/TnJ8Hz4/yYP68POSvHSirS/qw49M8o/zfj33Wo5J8pyJdXddkp/sz8PZSV7dn5fDkry1z/f2JA/ow7dIsmne7diJth7U3w8/M3H7pROv94X1/iNJPpNk78XzrIe/LP1ev2tfRzfqt/2/JE+aeN3+0PspyQlJ/qGv97sluahPf9DEer91ki+txvpOcnSGveeLpz8uyfsz/KTb7TL0C7fPsPf1HRPzHZ/kyX344iTP7MP/X5JXLl7fffz1SR7Yhw9M8tmJ+c5OcrOJ98FkH3ZakoP78H9K8oElXk+nZ6Jvm6KdSz6vGfb8Xp5kn2ztcw6dYp3+6sSyT88K/WofvyTJnRa91o/J0F//TobfiL/Jdtp/Qoa+cs9ZvbaT3DPJm5PcNMm5k6+D/lwdP1HLG7O1r/pmtu3HDlmH79MtGfrbQ/ptb0ryxD68z8R9/yxbX9/bPOd9nX0kyY2S/FSS7yR5RL/tlGz9HL3NxPJem94HrEG7b9HX2z/31+yDJt6zv92HX5Lhc/OW/bV55TR9QYYvrM9OcmCf/y8mnr+9+mPefE7re6V2/68+/KSJ1/Le2Xqx1qdl67bCVOt3Tm1crm/blORWfXjfJBf19+XjkvztxHy37v9Pz9ZtyeU+n96e5Mg+/JvpfecatfOJSV7Vhz+W5N6Z+IzI8F4+Of1zMcltMvRXlyT58T7tpCTPmngNLPV5dVySP+nDD0lybh9+cno/t0bt/XaSW/U6b51tt+GXXA9Z5X6pL+/xs2qjPbbbullVnZvkrAyd7KuSPDDJKa21f2+tfTvJW5Is7I37cmvtExP3/48MKz8ZOuQta1jn4np+JsMhJFe11q5L8rok/znJF5P8WFUdV1UPz7CBMGvL1ZKsznO2sNf2/kk+3v8Wxj+WIdj+QpJzMuy9u0uSg/t9j66q85J8IskBE9NvSPL3ffjvkjywqm6dIfR8qE8/caIdyfDa2JV2rIUvtdYuaK3dkOFLmtPa0NNckK01fzTJi6vq6AztvW4+pe6yf2mtnbnMbb/b1/vHk+yf5M5rV9YOWeq9/tAMX1Cd2W97aJIf6/Ov9H56a2vthjYcFna7JOmv5YOq6rYZ9pidPOP1/cAkb2itXd9auyLJhzL0D9szzXvrYUmO78/JqUluVVuPNji1tXbt4jv0b+vvn+Qf+v3+JsPG9S7ZzvP6/jYcQn9tb9cDs/I6vT7DhtViy/Wr903y4dbal3otX5u4z28keUSSx7XWvjdF+/+htXb9Lj0ZK2itnZ9hfR6e7f/U39sn+qorFvVjW2ZV45SW+0z+Uhv2ciXbvnbvUcPe8QuSHJFtT59Z/Jy/u7X2/Qzt3jPJe/r0yT775/oewQsybDRPLm9m+jbRTyc5KslVSf6++pEWGd6DC3We0Vr7VmvtqiTfreHImZX6grtm+DL6Ua21r/Rpv5Dkuf15Pj1DuFizPXuTttPuN0z8XzjCYv8k7+3r5/ey7fqZZv2uJ5XkL6rq/CT/mGS/DJ8nFyR5WN9j/bOttW8scd/lPp/ul+GLyWQIQA+cUe1LOTzDl2bp/w9fdPvDkrx8of/u/elPZHhv/3OfZ5ptwQdmaFtaax9Isk/fplxzrbVvZgjjRy+6aaX1MJp+ad0fK73Grm3DuXo/sJ1d5/++aPz7/YM3GTZGZvX8LlXn4nqWrLu1dk1V/VSSX0zyjCS/muGbmWldl20PYb/pFPdZ6Tlcjeds4Tzbn8ywB+SSJP89Q2h/dYZvgP9Pa+1vtimq6sEZOq37tda+U1WnZ/n2tGWmT/pe/z/Ldb+rvjcxfMPE+A3pNbfWnl9V78yw5/kTVfWw1trn1rbMVbH4/ZkkqaqHpQeA1tq1VfWRTPc6nofl+qQTW2u/v8T8K72fJtf95HvytRk2rn89O9YXrOTTSZa6OMRyfcH2+pVp3lt7ZHgvbxNgl+gbF9/n64uf4x2wXDuT5Z/XxX1Jy/C8LLdOv7tMuFzuuawlHmPBhUkOybCh/aVsv/3LPW+r6dQkL8zQT++zwnyTfdXifmze/e1yn8mTdV6fYS99MuyxeHRr7bweiB48Md/i53zhcNcbqmry/X1Dkk1VddMMew0Pba1dUsOFYNasP+uvzdOTnN43YI+crDvLr6+Vtgsuz9CGeyW5rE+rDF/IfH51Kt81K7R78r23MHxckhe31k7t2x3HTMyz4vqdTfVTWa5vOyLDnvefbq19v6ouTnLT1to/V9VPZ9hu+D9V9b7W2vMW3Xfa7b1ptrd2WVWwGS3lAAAGsElEQVTtkyFw3aOqWoaA1jK8n34w2xL1bO+Q2qU+r5a6z5q0cxkvzbCj5zUrzDNZ32j6JXtst+/DSR5dw1UOb57kMRnOoVnvzkjyoKrat4ZztQ5P8qEaThjfo7V2cpI/ynDYRZJ8K8OhQkv5fm091+OKDOen7dPPKfjlifmWW8aStexK4xb5aK/ja/3b369lOFTpfhn2yL03yW/2vROpqv36npRbJ7mmh9q7ZNjTsWCPbO3U/0uSj/RvIK+prefP/sYqt2NdqKo7970hL8iwB+Iu865pld06w2vl2qq6e6bbY7ienJbk8f01vHDuzh13YXknJHlWkrTWPr3r5SUZDou9SVX91sKEqvqZJNck+bUazlvanOELhk8m+XKSu1XVTfq32A+d4jEW9zfvy3CY7cLjbTes9m+uv1RVT+j3qf7F3/Yea8GS7ayqB2X55/Xn+zq7WYZDhz+anVuny/WrH+/T77SwrIn7nJPhlI1Tq+oOO9D+WXp1kue11i5Y48edp1smubx/rh6xi8ta2Fi8un/Gzexqo4tV1U9U1cETkw7J8F6exoezdF+QJF/PcO71X/QgmAyf489c2NlQVffa1fp31nba/WsT/z/eh2+drRdGOzLjsFwffscMh5N/v6p+ro+nhqvGf6e19ncZvqi69xLLXM7HsvX81iMyHOa6Fh6f5KTW2h1ba1taawdk+MJv/4l53pfk6dUvmtT7088l2VL9HOpMty344fT3en9NX93737no28lvynBu8YLVXA9z65fm/S3nutda+1QNF6dY6HBf2Vo7p4aLl6xbrbXLq+r3k3wwwzdF72qtva1vtLymtl41eWEPwQlJXl5V12bRBUoyHBJ0flV9qrV2RFU9L8NG1ZcyvMGz0jKWq2UVm3tBhnM9Xr9o2i1aa1cneV9V3TXJx/tn4rcznFfxngwd1vlJPp/hcOQF/57k7lV1dpJvZOuH1ZG9jT+S4bDup6xiO9aLZ/UPrOsznH/67jnXs9remeHiPudleP2eMed6dkhr7TNV9YcZXtd7JPl+hqMvpt2gXLy8K6rqs0neuoo1tqp6TJKXVtVzk3w3wzk9z8pwftp5Gb4N/h+tta8mSVW9KcO5eF/IEMC25+1J3lzDxUuemeGwqr/u7+dNGTYkprk67RFJXtaf0xtlOBztvEXznJCJvm1hr/BK7Vzhef1Ihr25ByV5fWvtrN7+HVqnK/WrNVwI7y19WVdmuOLwwv0+UsPP/ryzqn5+yvbPTGvt0iR/tVaPt078UYZ+58sZPqt2+gJ9rbWvV9Xf9uVcnGS50y9m4RZJjqvh0OLrMpxveVS2/cJ7Oadk2E7Ypi/oXzIv9EuPSvLuqvrNDNfOeGmGbZHK0NZpHmcWVmr3TWq40Nse2XpY6zEZDvf/1wzbGXda84p30Ap92zFJjq2qszKcZ7ywDfiTSf6yqm7I0H/99g483NFJXl1Vv5fh0O612q46PMnzF007Odv+Esgrk/x4htfd9zOcR3x8VT0lwzrdlOE99/Ks7JgM297nZzgndT18wfGiTHwZnFVcD/PslxZOZgcmVNW3W2u3mHcdMGv9S5oLMvx0xVLnRbETPK+wsdRwWO6h/Qt1YA4cigywQdVwvvHnkhwnfK0ezysArD17bAEAABg1e2wBAAAYNcEWAACAURNsAQAAGDXBFgDWkao6pv8kDwAwJcEWAACAURNsAWCOqupJVXV+VZ1XVa9ddNtvVdWZ/baT++/jpqqeUFUX9ukf7tPuXlWfrKpz+/IOnkd7AGAe/NwPAMxJVd09yVuSPKC1dnVV3SbJ0Um+3Vp7YVXt01r7tz7vnyW5orV2XFVdkOThrbV/raq9Wmtfr6rjknyitfa6qrpxkj1ba9fOq20AsJbssQWA+XlIkje31q5Oktba1xbdfo+q+qceZI9Icvc+/aNJTqiq30qyZ5/28SR/UFX/M8kdhVoANhLBFgDmp5KsdOjUCUl+p7X2k0n+NMlNk6S19vQkf5jkgCTn9j27r0/yK0muTfLeqnrILAsHgPVEsAWA+Tktya9W1T5J0g9FnnTLJJdX1Y0y7LFNn+/OrbUzWmt/nOTqJAdU1Y8l+WJr7dgkpya555q0AADWgU3zLgAANqrW2qer6s+TfKiqrk9yTpKLJ2b5oyRnJPlykgsyBN0k+ct+cajKEI7PS/LcJE+squ8n+WqS561JIwBgHXDxKAAAAEbNocgAAACMmmALAADAqAm2AAAAjJpgCwAAwKgJtgAAAIyaYAsAAMCoCbYAAACMmmALAADAqP3/pUojOBByu9sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Class Distribution\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.countplot(new_df['class'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset\n",
    "\n",
    "Clean text to remove irrelevant text such as non-letters, whitespace, HTML tags, and stopwords. Stopwords are commonly used words such as 'a', 'am', 'as', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text pre-processing for English sites only, other languages are not supported!\n",
    "def clean_text(raw_text, remove_stopwords=False):\n",
    "    # 1. remove HTML\n",
    "    # 2. remove non-letters\n",
    "    # 3. convert words to lower case\n",
    "    # 4. remove whitespace\n",
    "    # 5. chop sentences into words\n",
    "    # 6. optionally remove stop words (false by default)\n",
    "    # 7. return a long string of words\n",
    "\n",
    "    html_text = BeautifulSoup(raw_text, 'html.parser').get_text()\n",
    "    non_letters = re.sub('[^a-zA-Z]', ' ', html_text)\n",
    "    words = non_letters.lower()\n",
    "    words = ' '.join(words.split())\n",
    "    words = words.split()\n",
    "    if remove_stopwords:\n",
    "        # 7. return a long string of words\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Word lemmatization is a NLP technique used to transform a word into its root form, allowing grouping of similar words. E.g. the root form of 'walks', 'walked', 'walking' is equivalent to 'walk'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_lemmatization(text):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    def get_pos(word):\n",
    "        w_synsets = wordnet.synsets(word)\n",
    "\n",
    "        pos_counts = Counter()\n",
    "        pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "        pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "        pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "        pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "\n",
    "        most_common_pos_list = pos_counts.most_common(3)\n",
    "        return most_common_pos_list[0][0]\n",
    "    \n",
    "    return [lemm.lemmatize(word, pos=get_pos(word)) for word in nltk.word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read & Clean Dataset (in Pandas)\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df = df.dropna()\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([w for w in x.split(' ') if len(w) > 1]))\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(word_lemmatization(x)))\n",
    "\n",
    "# Features\n",
    "X = df['text'].values\n",
    "# Response\n",
    "y = df['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split\n",
    "\n",
    "Splitting the data into training and testing sets is essential for evaluating predictive model's performance. The training set will be used to train the model while the testing data is used to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is an information retrieval technique used to normalize the frequencies of word occurences by scaling down the impact of words that frequently appears. \n",
    "\n",
    "The machine learning algorithm I used are _Linear Support Vector Classifier_ and _Multinomial Naive Bayes_.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "Perform search on various hyperparameters to find the optimal set of parameters that enhances the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes (OneVsRest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Finish training in 4982.342403s\n",
      "Saved Model\n",
      "0.767975830816\n",
      "{'clf__estimator__alpha': 1, 'clf__estimator__fit_prior': True, 'tfidf__encoding': 'latin-1', 'tfidf__lowercase': True, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__stop_words': 'english', 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='latin-1', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...Classifier(estimator=MultinomialNB(alpha=1, class_prior=None, fit_prior=True),\n",
      "          n_jobs=1))])\n"
     ]
    }
   ],
   "source": [
    "clf_pipeline = imbPipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('oversample', SMOTE(random_state=2)),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB()))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__encoding': ['latin-1'],\n",
    "    'tfidf__lowercase': [True],\n",
    "    'tfidf__stop_words': ['english'],\n",
    "    'tfidf__norm': [None, 'l1', 'l2'],\n",
    "    'tfidf__use_idf': [False, True],\n",
    "    'tfidf__smooth_idf': [False, True],\n",
    "    'tfidf__sublinear_tf': [False, True],\n",
    "    'clf__estimator__alpha': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'clf__estimator__fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "t0 = time()\n",
    "\n",
    "grid = GridSearchCV(clf_pipeline, parameters, n_jobs=-1, cv=5, scoring = 'accuracy')\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Finish training in %fs\" % (time() - t0))\n",
    "\n",
    "joblib.dump(grid, 'nb_model.pkl', protocol=-1)\n",
    "print('Saved Model')\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Alcohol       0.89      0.90      0.90        52\n",
      "      Casino       0.82      0.98      0.89        54\n",
      " Counterfeit       0.75      0.90      0.82        30\n",
      " Cyberlocker       0.80      0.80      0.80        25\n",
      "         Mlm       0.73      0.74      0.74        47\n",
      "      Normal       0.92      0.50      0.65       171\n",
      "       Penny       0.79      0.80      0.80        41\n",
      "      Pharma       0.87      0.96      0.91        55\n",
      "        Porn       0.78      0.82      0.80        56\n",
      "Prostitution       0.82      0.82      0.82        22\n",
      "       Smoke       0.98      0.88      0.93        51\n",
      "        Spam       0.43      0.97      0.60        36\n",
      "         Tms       0.59      0.84      0.70        19\n",
      "       Trial       0.07      0.10      0.08        10\n",
      "      Weapon       0.92      0.88      0.90        41\n",
      "\n",
      " avg / total       0.82      0.77      0.77       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('nb_model.pkl')\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Finish training in 30291.770368s\n",
      "Saved Model\n",
      "0.865256797583\n",
      "{'svm__estimator__C': 1, 'svm__estimator__class_weight': None, 'tfidf__encoding': 'latin-1', 'tfidf__lowercase': True, 'tfidf__ngram_range': (1, 1), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': False, 'tfidf__stop_words': 'english', 'tfidf__sublinear_tf': True, 'tfidf__use_idf': False}\n",
      "Pipeline(memory=None,\n",
      "    steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='latin-1', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=False...lti_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "          n_jobs=1))])\n"
     ]
    }
   ],
   "source": [
    "clf_pipeline = imbPipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('oversample', SMOTE(random_state=2)),\n",
    "    ('svm', OneVsRestClassifier(LinearSVC()))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__encoding': ['latin-1'],\n",
    "    'tfidf__lowercase': [True],\n",
    "    'tfidf__stop_words': ['english'],\n",
    "    'tfidf__norm': [None, 'l1', 'l2'],\n",
    "    'tfidf__use_idf': [False, True],\n",
    "    'tfidf__smooth_idf': [False, True],\n",
    "    'tfidf__sublinear_tf': [False, True],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    \"svm__estimator__C\": [0.01, 0.1, 1],\n",
    "    \"svm__estimator__class_weight\": ['balanced', None]\n",
    "}\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "t0 = time()\n",
    "\n",
    "grid = GridSearchCV(clf_pipeline, parameters, n_jobs=-1, cv=3, scoring='accuracy')\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Finish training in %fs\" % (time() - t0))\n",
    "\n",
    "joblib.dump(grid, 'svm_model.pkl', protocol=-1)\n",
    "print('Saved Model')\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Alcohol       0.96      0.94      0.95        52\n",
      "      Casino       0.96      0.96      0.96        54\n",
      " Counterfeit       0.72      0.97      0.83        30\n",
      " Cyberlocker       0.88      0.88      0.88        25\n",
      "         Mlm       0.78      0.77      0.77        47\n",
      "      Normal       0.85      0.89      0.87       171\n",
      "       Penny       0.89      0.80      0.85        41\n",
      "      Pharma       0.91      0.96      0.94        55\n",
      "        Porn       0.87      0.86      0.86        56\n",
      "Prostitution       0.81      0.77      0.79        22\n",
      "       Smoke       1.00      0.90      0.95        51\n",
      "        Spam       0.97      0.86      0.91        36\n",
      "         Tms       0.89      0.89      0.89        19\n",
      "       Trial       0.20      0.10      0.13        10\n",
      "      Weapon       0.86      0.88      0.87        41\n",
      "\n",
      " avg / total       0.88      0.88      0.88       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('svm_model.pkl')\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Metrics used to evaluate a classification model include True-Positive and False-Positive Rate of the model's predictions. The following below is a confusion matrix which depicts the True-Positve and False-Positive Rate of the model's predictions.\n",
    "\n",
    "The model performs generally well on most classes except for the 'Trial' class. This is due to the small amount of samples we have for the 'Trial' class. As such, to further improve the model performance, it is best to collect more data on the 'Trial' class.\n",
    "\n",
    "<img src=\"confusion_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Applications for NLP\n",
    "\n",
    "Here, I will be implementing a deep learning neural network architecture to perform classification of websites based on its text data. Similar to the machine learning pipeline shown previously, I will be preprocessing the text and splitting the data into train and test datasets.\n",
    "\n",
    "The neural network I will be building here is **LSTM** (Long-Short-Term Memory), a type of recurrent neural network. It takes into account the sequence of the inputs, by feeding the output from the previous step as input to the current step. \n",
    "\n",
    "In the neural network architecture I am building, I will also be utilizing a pre-trained word embedding model (Glove). Word embeddings are essentially vector representations of text; it is able to capture context of a word in a document, semantic similarity, and relation with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import (Dense, Dropout, Embedding, Input, Flatten, Conv1D, BatchNormalization, Activation,\n",
    "                          GlobalMaxPooling1D, MaxPooling1D, LSTM, Bidirectional)\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Word Embedding (Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = loadGloveModel('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(texts, MAX_WORDS, MAX_SEQUENCE_LENGTH):\n",
    "    '''Preprocess text: Tokenize text to sequence; Pad sequence of text data\n",
    "    Returns processed text data'''\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    return data, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels, VALIDATION_SPLIT):\n",
    "    '''Splits data based on validation size and one-hot encodes labels\n",
    "    Returns: X_train, X_test, Y_train, Y_test\n",
    "    '''\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=VALIDATION_SPLIT, \n",
    "                                                        stratify=labels, shuffle=True, random_state=42)\n",
    "\n",
    "    Y_train = to_categorical(np.asarray(Y_train))\n",
    "    Y_test = to_categorical(np.asarray(Y_test))\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "texts = df['text']\n",
    "labels = df['class'].factorize()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_layer(glove, word_index, EMBEDDING_DIM):\n",
    "    '''Prepares embedding matrix using pre-trained word vector (GLOVE) and \n",
    "    builds an Embedding layer using the matrix. Returns Embedding Layer'''\n",
    "    \n",
    "    # prepare embedding matrix\n",
    "    num_words = min(MAX_WORDS, len(word_index)) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_WORDS:\n",
    "            continue\n",
    "        embedding_vector = glove.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    # note that we set trainable = False so as to keep the embeddings fixed\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                embeddings_initializer=Constant(embedding_matrix),\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model():\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "    x = Dense(256, activation='relu')(l_lstm)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(15, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 80000\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "VALIDATION_SPLIT = 0.25\n",
    "EMBEDDING_DIM = 300\n",
    "filepath = 'model_data/glove_lstm_model_weights_v1.hdf5'\n",
    "\n",
    "data, word_index = preprocess_text(texts, MAX_WORDS, MAX_SEQUENCE_LENGTH)\n",
    "X_train, X_test, Y_train, Y_test = split_data(data, VALIDATION_SPLIT)\n",
    "embedding_layer = get_embedding_layer(glove, word_index, EMBEDDING_DIM)\n",
    "model = lstm_model()\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "optimizer = Adam(lr=0.001)\n",
    "steplr = ReduceLROnPlateau(min_lr=0.0001, patience=5, factor=0.2)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "history = model.fit(X_train, Y_train, batch_size=6, epochs=10, validation_data=(X_test, Y_test), \n",
    "                    callbacks=[checkpoint, steplr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
